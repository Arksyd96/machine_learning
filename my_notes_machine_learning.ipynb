{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine learning**\n",
    "\n",
    "### *Introduction*\n",
    "- Machine learning helps to reproduce tasks made by humans and those we cannot program by hand. It is also used for data mining and self-customizing programs (Customizing preferences and data).\n",
    "- ML gives the capability to machines to learn without being explicitly programmed.\n",
    "- Types of ML :\n",
    "  - Supervised learning: We're given a dataset with labels. In other words, we already know what our output should look like. Supervised learning is categorized into :\n",
    "    1. Regression problems: Predicting output within a continuous domain of definition (Mapping to a continuous function).\n",
    "    2. Classification problems: Predicting output in a discrete domain of definition (Mapping to discrete categories can be seen as a step function).\n",
    "  - Unsupervised learning: We're given a dataset without knowing how our results should look and don't necessarily know the effect of each variable. A solution is usually to cluster data based on relationships among the variables. With unsupervised learning, there's no feedback on the prediction results.\n",
    "  - Other: Reinforcement learning, recommender systems.\n",
    "\n",
    "# Linear regression :\n",
    "- Given a training set, our primary goal is to learn a mapping function $h: x \\rightarrow y$ which can predict correctly (or approximately) a value of $y$ given $x$ as input ($x, y \\in \\mathbb{R}$).\n",
    "- By default, we define the hypothesis function as follow : $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$ where $\\theta_{i}$ are parameters.\n",
    "- $h(x)$ is a function for univariate linear regression (with one variable as input).\n",
    "Cost function measures the loss between the output of $h(x)$ and the ground truth value $y$.\n",
    "- Most commonly used cost function for regression problems is the \"Mean squared error\" and is defined as follow :\n",
    "$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_1^m (\\hat{y} - y)^2 $$  \n",
    "\n",
    "- Where $m$ is the number of training examples and $\\hat{y}$ is the predicted output. We put $\\frac{1}{2}$ on the MSE to simplify the computation of the gradient descent. When derived, it cancels the square.\n",
    "\n",
    "#### 1. Gradient descent:\n",
    "- We modify the values of the parameters by subtracting its gradient each time until we get a function $h$ that predicts well. In other words, finding a combination of parameters $\\theta_{0}, \\theta_{1}, ..., \\theta_{n}$ that minimizes the cost as much as possible. In some cases, we run into local minimums when dealing with complex functions; one possible solution is to restart the learning phase with another distribution of parameters.  \n",
    "``Repeat until convergence:``\n",
    "$$ \\theta_{j} := \\theta_{j} - \\eta \\times \\frac{\\partial}{\\partial \\theta_{j}}J(\\theta_{0}, \\theta_{1},...,\\theta{n}) $$  \n",
    "Where $\\eta$ is the learning rate. This parameter has an impact on the training speed. A substantial value means faster learning but can sometimes cause oscillations and divergence. In the other case, if $\\eta$ is too small, then gradient descent will be slow.\n",
    "- *Note*: All the parameters should be updated at once.\n",
    "- One possible solution if the cost starts growing after $k$ iterations is to reduce the value of the learning rate $\\eta$.\n",
    "If $\\eta$ is sufficiently small, the cost should decrease on every iteration, but the gradient descent will hardly converge.\n",
    "- *Note*: Some other gradient-based optimization algorithms: Conjugate gradient, BFGS, L-BFGS (more complex).\n",
    "\n",
    "#### 2. Multivariate linear regression (Linear algebra):\n",
    "- let $n$ be the number of input features. If $n > 1$, then we're working on multivariate linear regression problem :\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$\n",
    "- By vectorizing our hypothesis function, we'll be able to reduce the computing time and apply gradient descent on all training examples at once. This method is called '**Batch gradient descent (BGD)**' (Update is made on a whole batch of examples).  \n",
    "$$\\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},\n",
    "x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, \n",
    "h_{\\theta}(x) = \\theta^{T}x$$  \n",
    "\n",
    "- In more details, here's what the **BGD** formula looks like : $ \\theta_{j} := \\theta_{j} - \\frac{\\eta}{m} \\sum_i^m \\frac{\\partial}{\\partial \\theta_{j}}J^{(i)}(\\theta_{0}, \\theta_{1},..., \\theta_{n}) $ where $\\frac{\\partial J}{\\partial \\theta_{j}}$ formula can be found as follow :  \n",
    "$$\\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{\\partial J}{\\partial h}\\frac{\\partial h}{\\partial \\theta_{j}} = (h_{\\theta}(x) - y)x_{j}$$\n",
    "$$ \\theta_{j} := \\theta_{j} - \\frac{\\eta}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} $$\n",
    "\n",
    "#### 3. Features scaling (z-score normalization):\n",
    "- When dealing with multivariate regression problems, it is crucial to rescale all the features to the same range around 0, especially when we have different units. Variables measured at different scales do not contribute equally to the analysis and might create a bias. A variable that ranges between 0 and 1000 will outweigh a variable that ranges between 0 and 1.\n",
    "- One way to rescal input features is to apply the formule :\n",
    "$$ x_{i} := \\frac{x_{i} - \\mu_{i}}{s_{i}} $$  \n",
    "Where $\\mu_{i}$ is the average value of the feature $x_{i}$ and $s_{i}$ is the standard deviation ``max - min``.\n",
    "\n",
    "#### 4. Normal equations :\n",
    "- Another method to minimize the function $\\partial J$ without using an iterative algorithm like the gradient descent is to use the '**Normal equation**'. In this method, we minimize the cost by resolving the problem $\\frac{\\partial J}{\\partial \\theta} = 0$ and by simplifying this equation, we obtain the normal equation formula:\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$ \n",
    "- I explain in more details the demonstration of this function [here](https://github.com/Arksyd96/machine_learning/blob/main/normal_equation_demonstration.ipynb) ([NBviewer version](https://nbviewer.jupyter.org/github/Arksyd96/machine_learning/blob/main/normal_equation_demonstration.ipynb)).\n",
    "- The normal equation has some advantages and disadvantages over the default method, which is gradient descent. Here are some points (From Andrew Ng's course):\n",
    "  - **Gradient descent**:\n",
    "    - Pros:\n",
    "      - Complexity $O(knÂ²)$.\n",
    "      - Works well when the number of features is large.\n",
    "    - Cons:\n",
    "      - Features scaling is necessary.\n",
    "      - Needs many iterations.\n",
    "      - Needs to define a learning rate $\\eta$.\n",
    "  - **Normal equation**:\n",
    "    - Pros:\n",
    "      - No iteration.\n",
    "      - No need for a learning rate.      \n",
    "    - Cost:\n",
    "      - Computing inverse of $X^TX$ has a complexity of $O(n^3)$.\n",
    "      - Slow when dealing with a lot of features\n",
    "- *Note:* Features scaling is unnecessary when working with normal equations; keeping the features as they are won't affect the results.\n",
    "- It is recommended to use **Normal equation** when the number of features $n$ does not exceed $10^{5 to 6}$; otherwise, gradient descent is better. \n",
    "- *Note:* In some cases, the term $X^TX$ is noninvertible. This happens if:\n",
    "  - There are redundant or/and linearly dependent features.\n",
    "  - Too many features $n > m$.\n",
    "\n",
    "# Polynomial regression:\n",
    "- Sometimes, the repartition of our training data forms a non-linear curve, and our hypothesis function cannot fit all these data. To deal with non-linear problems, we have to use polynomial regression.\n",
    "- The idea behind polynomial regression is to compose our input features with non-linear functions then create new additional features based on those compositions. Here are some examples:\n",
    "  - Quadratic function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{1}^2$\n",
    "  - Cubic function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{1}^2 + \\theta_{3}x_{1}^3$\n",
    "  - Root squared function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}\\sqrt{x_{1}}$\n",
    "- One important thing to keep in mind is if you choose your features this way, then feature scaling becomes very important. Do not forget to normalize your inputs.\n",
    "\n",
    "# Classification:\n",
    "- Given a training set, our goal is to predict to which class a training example belongs. Unlike the logistic regression hypothesis function, in classification, we predict discrete values $y \\in \\{0, 1, ..., n_{class}\\}$.\n",
    "- When the number of output classes $n_{class}$ is equal to $2$, we call it a binary classification problem, and outputs are either positive ($1$) or negative ($0$). \n",
    "- One approach is to use linear regression and map predictions greater than 0.5 as a 1 and lesser than 0.5 as a 0. However, this method doesn't work well because classification is not a linear function (By Andrew Ng). On possible alternative is to use **Logistic regression**.\n",
    "\n",
    "#### 1. Logistic regression:\n",
    "- **Logistic regression** is the most commonly used technique for binary classification. Our hypothesis function needs to be modified because the default outputs value greater than 1 and lower than 0. In order to obtain $\\hat{y} \\in \\{0, 1\\}$ we will combine our hypothesis function with a logistic function to satisfy $0 \\leq h_{\\theta}(x) \\leq 1$. The output will then be interpreted as a probability.\n",
    "$$ h_{\\theta}(x) = P(y = 1|x;\\theta) \\in [0, 1] $$\n",
    "$$  h_{\\theta}(x) = \\sigma(\\theta^Tx), \\;\\; where \\;\\; \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "- The function $\\sigma$ is also called sigmoid function. This function maps any real number to the interval $[0, 1]$ which makes it useful for binary classification.\n",
    "- The sigmoid is a non-linear function, combined with $h_{\\theta}(x)$, it allows us solve complex problems that require non-linear functions.\n",
    "\n",
    "\n",
    "\n",
    "##### Decision boundary:\n",
    "- Decision boundary (or decision surface) is a function that delimits the region of each class in the data space. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable ([Wikipedia](https://en.wikipedia.org/wiki/Linear_separability)).\n",
    "- To find the decision boundary function of a binary classification problem:\n",
    "$$ given \\;\\; h_{\\theta}(x) = \\sigma(z) \\;\\; where \\;\\; z = \\theta^Tx $$\n",
    "$$ y = 1 \\;\\; if \\;\\; h_{\\theta}(x) \\geq 0.5 \\Longrightarrow z \\geq 0 $$\n",
    "$$ y = 0 \\;\\; if \\;\\; h_{\\theta}(x) < 0.5 \\Longrightarrow z < 0 $$\n",
    "$$ \\theta^Tx = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n = 0 $$\n",
    "\n",
    "- Unlike regression problems, we cannot use the 'Mean squared error' cost function because when combined with a sigmoid, the output function will be non-convex and, thus, will contain many local optima. Instead, we use the cross-entropy cost function, which looks like this:\n",
    "$$ J(\\theta) = \\begin{cases}y=1 & -log(h_{\\theta}(x))\\\\y=0 & -log(1 - h_{\\theta}(x))\\end{cases}  $$\n",
    "- If the model makes a mistake (predicts $1$ when $y = 0$ or predict $0$ when $y = 1$), the value of the cost will converge to $\\infty$.\n",
    "- If the model makes a good prediction, the cost value will converge to $0$.\n",
    "When combined with the sigmoid, the cost function will be convex (Function with a bowl form, with only 1 optimum).\n",
    "- A more simplified way to write the cost function is to group the two terms of $J(\\theta)$ into one case:\n",
    "    - $when \\;\\; y = 1 \\Longrightarrow J(\\theta) = -log(h_{\\theta}(x^{(i)}))$.\n",
    "    - $when \\;\\; y = 0 \\Longrightarrow J(\\theta) = -log(1 - h_{\\theta}(x^{(i)}))$.\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))]$$\n",
    "- This loss function can be derived from statistics using the principle of **Maximum likelihood estimation**. Which is an idea in statistics for how to efficiently find parameters data for different models. I explain more in details the demonstration of this function [here]()([NBviewer version]()).\n",
    "- When deriving $J(\\theta)$ with respect to $\\theta_j$, we obtain the same function as in regression and which is convex:\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} $$\n",
    "- *Note*: Since the derivatives of cost functions for regression and logistic regression are the same. One may wonder what is the difference then? The difference is lies in the hypothesis function, where for logistic regression, is combined with a sigmoid $h_{\\theta}(x) = \\frac{1}{1 + e^{\\theta^Tx}}$.\n",
    "- *Note*: Vectorized implementation for gradient descent:\n",
    "$$ \\theta := \\theta - \\frac{\\eta}{m}X^T(\\sigma(X\\theta) - Y) $$\n",
    "\n",
    "#### 2. One-vs-all method for multiclass classification:\n",
    "- A multiclass classification is a classification problem that satisfies $\\;n_{class} > 2$ and $y \\in \\{1, 2, ..., n_{class}\\}$.\n",
    "- One-vs-all method consists on dividing a multiclass classification problem into $n_{class}$ logistic regression models. For each class, we denote its data as being the first class, and we group the rest of the data in the second one:\n",
    "$$ prediction = arg\\;max(h_{\\theta}^{(0)}(x), h_{\\theta}^{(1)}(x), h_{\\theta}^{(2)}(x), ..., h_{\\theta}^{(n_{class})}(x)) $$ \n",
    "- We'll pick the class whose hypothesis function offers the highest probability. \n",
    "\n",
    "# Overfitting and underfitting:\n",
    "1. **Underfitting** (or high bias problem) is when our model maps poorly the data because our hypothesis function is too simple to fit complex problems or uses too few features. Try one of the below solutions :\n",
    "    - Train the model for few more iterations.\n",
    "    - Try bigger learning rate values.\n",
    "    - Add extra features $(x^n, \\sqrt{x}$. etc $)$.\n",
    "    - Add some training data.\n",
    "2. **Overfitting** (or high variance problem) is a state where the model performs so well $(J(\\theta) \\simeq 0)$ on training data but fails to generalize when given new examples. We also call the problem as the **High variance** problem. Here are some approaches for solving the overfitting problem :\n",
    "    - Restart training with fewer iterations.\n",
    "    - Try smaller learning rate values.\n",
    "    - Reduce the number of features by keeping only the important ones.\n",
    "    - Use model selection algorithms.\n",
    "    - Regularization.  \n",
    "    \n",
    "Check my article on [Bias-Variance tradeoff]() for more formal details about overfitting and underfitting.\n",
    "\n",
    "# Regularization:\n",
    "- Regularization is a set of techniques that are used to prevent overfitting. There are many new ways to do regularization, we'll see here the simplest and most common one.\n",
    "- Regularization allows to reduce the complexity of the model by adding a penalty term to the cost function. In this way, we can keep our polynomial features to fit complex problems without overfitting.\n",
    "\n",
    "Suppose we're overfitting with the following hypothesis function:\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3 + \\theta_4x^4 $$  \n",
    "The idea behind regularization is to reduces values of $\\theta_j$ of higher polynomial features in order to make the function *simpler* (By minimizing $\\theta_j$, we reduce the slope and the function will be more linear).  \n",
    "Here's the loss function with regularization:\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "Where $\\lambda$ is the regularization parameter and is generally set to a small value (e.g. $\\lambda = 0.1$). The value of the hyperparameter $\\lambda$ is crucial, it has a direct impact on the balance between the cost function and the regularization term, those two terms are combined in the cost function and should be tuned accordingly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization applied to linear regression:\n",
    "By applying the regularization term we've seen earlier, the gradient descent function becomes as follow:  \n",
    "$$\\theta_0 := \\theta_0 - \\frac{\\eta}{m}\\sum^m_{i=0}(h_{\\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j := \\theta_j - \\frac{\\eta}{m}\\sum^m_{i=0}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j \\hspace{1cm} j \\in \\{1, 2, ..., n_{features}\\}$$\n",
    "\n",
    "By grouping $\\theta_j$, the second formula can be summarized as:\n",
    "$$\\theta_j := \\theta_j(1 - \\eta\\frac{\\lambda}{m}) - \\frac{\\eta}{m}\\sum^m_{i=0}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "The term $(1 - \\eta\\frac{\\lambda}{m})$ is also called the **regularization term** and is always less than $1$. This form is easier to implement since the second term is always the same.\n",
    "\n",
    "This regularization term can also be applied on the normal equation, it is simply a regularization term that is added to the diagonal of the matrix $\\mathbf{X}^T\\mathbf{X}$ :\n",
    "$$\\theta = (X^TX + \\lambda\n",
    "\\begin{bmatrix}\n",
    "0 &  &  &  & & \\\\\n",
    " & 1 &  &  & & \\\\\n",
    " &  & 1 &  & & \\\\\n",
    " &  &  & . & & \\\\\n",
    " &  &  &  & . & \\\\\n",
    " &  &  &  & & 1\\\\\n",
    "\\end{bmatrix})^{-1}X^Ty$$\n",
    "\n",
    "\"Recall that if $m < n$, then $X^TX$ is non-invertible (singular or degenerate). However, when we add the regularization term, then $X^TX + \\lambda \\cdot L$ becomes invertible.\" - Andrew.\n",
    "\n",
    "### Regularization applied to logistic regression:\n",
    "Recall logistic loss function, and adding to it the regularization term:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})\\log(1 - h_{\\theta}(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "We exclude the first parameter $\\theta_0$ from the sum since it's the bias term. We update the weights as the same way as in linear regression: \n",
    "$$\\theta_0 := \\theta_0 - \\frac{\\eta}{m}\\sum^m_{i=0}(h_{\\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j := \\theta_j - \\frac{\\eta}{m}\\sum^m_{i=0}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j \\hspace{1cm} j \\in \\{1, 2, ..., n_{features}\\}$$\n",
    "\n",
    "except that we use the logistic function instead of the linear function.\n",
    "$$h_{\\theta}(x) = \\frac{1}{1 + \\exp^{\\theta^TX}}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
