{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demonstration of the normal equation formula:\r\n",
    "0. *We define the hypothesis and cost function as follow*:\r\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 $$ \r\n",
    "$$ \\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},\r\n",
    "x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, \r\n",
    "h_{\\theta}(x) = \\theta^{T}x$$  \r\n",
    "\r\n",
    "- *Note*: Our objective is to find the value of $\\theta$ that solves the equation $\\frac{\\partial J}{\\partial \\theta} = 0$.\r\n",
    "1. First we start by simplyfing the formula:\r\n",
    "    - Remove the $\\frac{1}{2m}$ since it has no impact when deriving.\r\n",
    "    - Eliminate the sum by introducing $(h - y)$ as a design matrix (Matrix that contains everything, it's shape will be $(n_{features}, m)$).\r\n",
    "$$ J(\\theta) = (\\begin{bmatrix} \\theta^Tx^{(1)} \\\\ \\theta^Tx^{(2)} \\\\ ... \\\\ \\theta^Tx^{(m)} \\end{bmatrix} - \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{bmatrix})Â²$$ \r\n",
    "$$ J(\\theta) = (\\begin{bmatrix} \\theta_0 x_0^{(1)} & \\theta_1 x_1^{(1)} & ... & \\theta_{n} x_n^{(1)} \\\\ \\theta_{0} x_0^{(2)} & \\theta_1x_1^{(2)} & ... & \\theta_{n} x_n^{(2)} \\\\ . & . & . & . \\\\ \\theta_0x_0^{(m)} & \\theta_1x_1^{(m)} & ... & \\theta_{n} x_n^{(m)}\\end{bmatrix} - \\begin{bmatrix}y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}\\end{bmatrix})^2 $$\r\n",
    "- *Note* that dimension of $x$ written this way becomes $m \\times n$ instead of $n \\times m$. This allows use to rewrite it in a more general form:\r\n",
    "$$ J(\\theta) = (X\\theta - Y)^2$$\r\n",
    "2. Deploy the binomial square identity (Be careful, this is the matrix version so we have to transpose the first term in order to square all the numbers):\r\n",
    "\r\n",
    "$$ J(\\theta) = (X\\theta - Y)^T(X\\theta - Y) = (X\\theta)^TX\\theta - (X\\theta)^TY - Y(X\\theta)^T + Y^TY$$\r\n",
    "3. Sum the $2nd$ and $3rd$ terms, $(X\\theta)^T$ and $Y$ are vectors. if $A$ and $B$ are vector matrices, then $A^TB = B^TA$ (Take care of this information, you'll need it a lot when dealing with linear algebra in machine learning).\r\n",
    "$$ J(\\theta) = (X\\theta)^TX\\theta - 2(X\\theta)^TY + Y^TY $$\r\n",
    "4. The formula as it is now cannot be more simplified. So we'll start searching for it's derivative with respect to $\\theta$:\r\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} - \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} + \\frac{\\partial Y^TY}{\\partial \\theta} $$\r\n",
    "$$ \\frac{\\partial Y^TY}{\\partial \\theta} = 0 $$\r\n",
    "$$ \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} = 2X^TY$$ \r\n",
    "$$ \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} = X^TX\\theta + X(X\\theta)^T = 2X^TX\\theta$$\r\n",
    "5. The initial objective was to resolve $\\frac{\\partial J}{\\partial \\theta} = 0$, so we put each term together:\r\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = 2X^TX\\theta - 2X^TY = 0$$\r\n",
    "$$ 2X^TX\\theta = 2X^TY $$\r\n",
    "$$ X^TX\\theta = X^TY $$\r\n",
    "6. And finally, the fraction $\\frac{1}{A}$ where $A$ is a matrix, is equivalent to the inverse of $A$. The inverse of a matrix is noted $A^{-1}$ and $AA^{-1} = I$.\r\n",
    "$$ \\theta = (X^TX)^{-1}X^TY $$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}