{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the normal equation formula:\n",
    "0. *We define the hypothesis and cost function as follow*:\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 $$ \n",
    "$$ \\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},\n",
    "x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, \n",
    "h_{\\theta}(x) = \\theta^{T}x$$  \n",
    "\n",
    "- *Note*: Our objective is to find the value of $\\theta$ that solves the equation $\\frac{\\partial J}{\\partial \\theta} = 0$.\n",
    "1. First we start by simplyfing the formula:\n",
    "    - Remove the $\\frac{1}{2m}$ since it has no impact when deriving.\n",
    "    - Eliminate the sum by introducing $(h - y)$ as a design matrix (Matrix that contains everything, it's shape will be $(n_{features}, m)$).\n",
    "$$J(\\theta) = (\\begin{bmatrix} \\theta^Tx^{(1)} \\\\ \\theta^Tx^{(2)} \\\\ ... \\\\ \\theta^Tx^{(m)} \\end{bmatrix} - \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{bmatrix})Â²$$ \n",
    "$$ J(\\theta) = (\\begin{bmatrix} \\theta_0 x_0^{(1)} & \\theta_1 x_1^{(1)} & ... & \\theta_{n} x_n^{(1)} \\\\ \\theta_{0} x_0^{(2)} & \\theta_1x_1^{(2)} & ... & \\theta_{n} x_n^{(2)} \\\\ . & . & . & . \\\\ \\theta_0x_0^{(m)} & \\theta_1x_1^{(m)} & ... & \\theta_{n} x_n^{(m)}\\end{bmatrix} - \\begin{bmatrix}y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}\\end{bmatrix})^2 $$\n",
    "- *Note* that dimension of $x$ written this way becomes $m \\times n$ instead of $n \\times m$. This allows use to rewrite it in a more general form:\n",
    "$$ J(\\theta) = (X\\theta - Y)^2$$\n",
    "2. Deploy the binomial square identity (Be careful, this is the matrix version so we have to transpose the first term in order to square all the numbers):\n",
    "\n",
    "$$ J(\\theta) = (X\\theta - Y)^T(X\\theta - Y) = (X\\theta)^TX\\theta - (X\\theta)^TY - Y(X\\theta)^T + Y^TY$$\n",
    "3. Sum the $2nd$ and $3rd$ terms, $(X\\theta)^T$ and $Y$ are vectors. if $A$ and $B$ are vector matrices, then $A^TB = B^TA$ (Take care of this information, you'll need it a lot when dealing with linear algebra in machine learning).\n",
    "$$ J(\\theta) = (X\\theta)^TX\\theta - 2(X\\theta)^TY + Y^TY $$\n",
    "4. The formula as it is now cannot be more simplified. So we'll start searching for it's derivative with respect to $\\theta$:\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} - \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} + \\frac{\\partial Y^TY}{\\partial \\theta} $$\n",
    "$$ \\frac{\\partial Y^TY}{\\partial \\theta} = 0 $$\n",
    "$$ \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} = 2X^TY$$ \n",
    "$$ \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} = X^TX\\theta + X(X\\theta)^T = 2X^TX\\theta$$\n",
    "5. The initial objective was to resolve $\\frac{\\partial J}{\\partial \\theta} = 0$, so we put each term together:\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = 2X^TX\\theta - 2X^TY = 0$$\n",
    "$$ 2X^TX\\theta = 2X^TY $$\n",
    "$$ X^TX\\theta = X^TY $$\n",
    "6. And finally, the fraction $\\frac{1}{A}$ where $A$ is a matrix, is equivalent to the inverse of $A$. The inverse of a matrix is noted $A^{-1}$ and $AA^{-1} = I$.\n",
    "$$ \\theta = (X^TX)^{-1}X^TY $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
